# Visual RAG Chain Flow Diagram

## ğŸ¯ Complete Flow Visualization

```
                           ğŸŸ¦ USER INPUT
                                |
                                v
                    ğŸ” CONVERSATION TYPE DETECTION
                           /        |        \
                          /         |         \
                         v          v          v
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ ğŸ—¨ï¸ REGULAR  â”‚ â”‚ ğŸ“ USER   â”‚ â”‚ ğŸ¢ GLOBAL   â”‚
                â”‚ CHAT        â”‚ â”‚ FILES     â”‚ â”‚ COLLECTION  â”‚
                â”‚             â”‚ â”‚ RAG       â”‚ â”‚ RAG         â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       |             |              |
                       v             v              v
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ LLM SERVICE     â”‚ â”‚ RAG SERVICE  â”‚ â”‚ RAG SERVICE  â”‚
            â”‚ llm_service.py  â”‚ â”‚ rag_service  â”‚ â”‚ rag_service  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       |             |              |
                       v             v              v
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ PROMPT:         â”‚ â”‚ PROMPT:      â”‚ â”‚ PROMPT:      â”‚
            â”‚ regular_chat_   â”‚ â”‚ user_        â”‚ â”‚ global_      â”‚
            â”‚ prompt          â”‚ â”‚ collection_  â”‚ â”‚ collection_  â”‚
            â”‚                 â”‚ â”‚ rag_prompt   â”‚ â”‚ rag_prompt   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       |             |              |
                       v             |              |
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      v              v
            â”‚ DIRECT LLM      â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ RESPONSE        â”‚ â”‚ CONTEXTUALIZATION STEP   â”‚
            â”‚                 â”‚ â”‚ (English Prompt)         â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ "Given chat history..."  â”‚
                       |        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       |                     |
                       |                     v
                       |        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       |        â”‚ VECTOR SEARCH            â”‚
                       |        â”‚ â€¢ User Collection        â”‚
                       |        â”‚ â€¢ Global Collection      â”‚
                       |        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       |                     |
                       |                     v
                       |        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       |        â”‚ CONTEXT ASSEMBLY         â”‚
                       |        â”‚ Retrieved Docs + Prompt  â”‚
                       |        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       |                     |
                       v                     v
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚           ğŸ¤– LLM GENERATION                 â”‚
            â”‚                                             â”‚
            â”‚  âš¡ STREAMING     or    ğŸ’¾ NON-STREAMING    â”‚
            â”‚  â€¢ astream()              â€¢ ainvoke()       â”‚
            â”‚  â€¢ Token by token         â€¢ Complete resp   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    |
                                    v
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚           ğŸ’¾ SAVE TO DATABASE               â”‚
            â”‚  â€¢ User message                             â”‚
            â”‚  â€¢ Assistant response                       â”‚
            â”‚  â€¢ RAG context (if applicable)              â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    |
                                    v
                              ğŸ“¤ RETURN RESPONSE
```

## ğŸ”„ Detailed Flow Breakdown

### ğŸ—¨ï¸ Flow 1: Regular Chat (No RAG)
```
User: "Hello, how are you?"
  â†“
Type Detection: No collection, no files â†’ REGULAR
  â†“
Service: llm_service.py
  â†“
Prompt: get_regular_chat_prompt(db)
  â†“
Fallback: None (basic LLM)
  â†“
LLM: Direct response
  â†“
Output: "Hello! I'm doing well, thank you for asking..."
```

### ğŸ“ Flow 2: User Files RAG
```
User: "Summarize my uploaded document"
Files: [user_123_resume.pdf]
  â†“
Type Detection: User files present â†’ USER_FILES
  â†“
Service: rag_service.py
  â†“
Collection: "user_123_documents"
  â†“
Contextualization: "Summarize my uploaded document" (no history)
  â†“
Collection Check: _is_global_collection() â†’ FALSE
  â†“
Prompt: get_user_collection_rag_prompt(db)
  â†“
Vector Search: In user_123_documents collection
  â†“
Context: Retrieved relevant sections from resume
  â†“
LLM: RAG response with personal document context
  â†“
Output: "Based on your uploaded resume, here's a summary..."
```

### ğŸ¢ Flow 3: Global Collection RAG
```
User: "What's our company policy on vacation?"
Collection: "company_policies"
  â†“
Type Detection: Global collection specified â†’ GLOBAL_COLLECTION
  â†“
Service: rag_service.py
  â†“
Collection: "company_policies" (admin-managed)
  â†“
Contextualization: "What's our company policy on vacation?"
  â†“
Collection Check: _is_global_collection() â†’ TRUE
  â†“
Prompt: get_global_collection_rag_prompt(db)
  â†“
Vector Search: In company_policies collection
  â†“
Context: Retrieved policy documents
  â†“
LLM: RAG response with company context
  â†“
Output: "According to our company policy documents..."
```

## ğŸš¨ Fallback Scenarios

### Scenario 1: Config Failure
```
User Input â†’ Type Detection â†’ Service Selection â†’ Prompt Retrieval FAILS
  â†“
Fallback: DEFAULT_RAG_SYSTEM_PROMPT
  â†“
Continue with default prompt
```

### Scenario 2: Collection Not Found
```
User Input â†’ RAG Service â†’ Vector Search FAILS
  â†“
Fallback: Continue with empty context
  â†“
LLM responds: "I don't have enough information..."
```

### Scenario 3: Complete RAG Failure
```
User Input â†’ RAG Service â†’ Multiple Failures
  â†“
Fallback: Return error message
  â†“
Output: "I'm having trouble accessing the knowledge base..."
```

## ğŸ›ï¸ Configuration Matrix

| Flow Type | Prompt Config Key | Default Available | Fallback |
|-----------|------------------|-------------------|----------|
| Regular Chat | `regular_chat_prompt` | âŒ No | None |
| User Files | `user_collection_rag_prompt` | âœ… Yes | DEFAULT_RAG_SYSTEM_PROMPT |
| Global Collection | `global_collection_rag_prompt` | âœ… Yes | DEFAULT_RAG_SYSTEM_PROMPT |

## ğŸ”§ Collection Detection Examples

### Global Collections (Returns TRUE):
- `company_docs`
- `admin_company_docs` 
- `knowledge_base`
- `admin_knowledge_base`

### User Collections (Returns FALSE):
- `user_123_documents`
- `user_456_files`
- `personal_collection`

### Regular Chat (No Collection):
- `null`
- `undefined`
- Empty string

## âš¡ Streaming vs Non-Streaming Comparison

| Aspect | Method Called | LLM Function | Output Format |
|--------|---------------|--------------|---------------|
| **Streaming** | `get_streaming_rag_response()` | `astream()` | Generator yielding tokens |
| **Non-Streaming** | `get_rag_response()` | `ainvoke()` | Complete response string |

**Both use identical logic for:**
- âœ… Contextualization
- âœ… Prompt selection  
- âœ… Vector search
- âœ… Context assembly
- âœ… Error handling

## ğŸ¯ Entry Point Routing

### Unified Chat API (`/chat`)
```python
# Auto-detects type based on:
if request.collection_name and is_global:
    â†’ Global Collection Flow
elif request.file_ids or existing_user_files:
    â†’ User Files Flow  
else:
    â†’ Regular Chat Flow
```

### Direct RAG API (`/rag`)
```python
# Always RAG, type based on collection:
if _is_global_collection(collection_name):
    â†’ Global Collection Flow
else:
    â†’ User Files Flow
```

This visual breakdown shows how the 3 chains work identically for streaming/non-streaming, with the key differentiator being the collection type and corresponding prompt selection.
